{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J002_Questions_Language.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOf61uMTcd+rpk9npKSKtqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/259mit/AI/blob/master/CS50/Language/J002_Questions_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trXiobZqPSRz",
        "outputId": "a202f71c-c30b-4097-83bf-bc09f6edca7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "import os\n",
        "import string\n",
        "import math\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdDE6YF3PYGt"
      },
      "source": [
        "FILE_MATCHES = 1\n",
        "SENTENCE_MATCHES = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNiUxvSQPYI3"
      },
      "source": [
        "def main():\n",
        "    # Calculate IDF values across files\n",
        "    files = load_files(sys.argv[1])\n",
        "    file_words = {\n",
        "        filename: tokenize(files[filename])\n",
        "        for filename in files\n",
        "    }\n",
        "    file_idfs = compute_idfs(file_words)\n",
        "\n",
        "    # Prompt user for query\n",
        "    query = set(tokenize(input(\"Query: \")))\n",
        "\n",
        "    # Determine top file matches according to TF-IDF\n",
        "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
        "\n",
        "    # Extract sentences from top files\n",
        "    sentences = dict()\n",
        "    for filename in filenames:\n",
        "        for passage in files[filename].split(\"\\n\"):\n",
        "            for sentence in nltk.sent_tokenize(passage):\n",
        "                tokens = tokenize(sentence)\n",
        "                if tokens:\n",
        "                    sentences[sentence] = tokens\n",
        "\n",
        "    # Compute IDF values across sentences\n",
        "    idfs = compute_idfs(sentences)\n",
        "\n",
        "    # Determine top sentence matches\n",
        "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
        "    for match in matches:\n",
        "        print(match)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIN-t6T2PYLS"
      },
      "source": [
        "def load_files(directory):\n",
        "    directory=(r\"/content/corpus\")\n",
        "    file_contents = dict()\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            f = open(os.path.join(root, file), \"r\")\n",
        "            file_contents[file] = f.read()\n",
        "\n",
        "    return file_contents"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DelWmy7PYPU"
      },
      "source": [
        "def tokenize(document):\n",
        "    punctuation = string.punctuation\n",
        "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "    words = nltk.word_tokenize(document.lower())\n",
        "    words = [word for word in words if word not in punctuation and word not in stop_words]\n",
        "\n",
        "    return words"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDyXZUnuPYTj"
      },
      "source": [
        "def compute_idfs(documents):\n",
        "    idfs = dict()\n",
        "    total_num_documents = len(documents)\n",
        "    words = set(word for sublist in documents.values() for word in sublist)\n",
        "    \n",
        "    for word in words:\n",
        "        num_documents_containing_word = 0\n",
        "        \n",
        "        for document in documents.values():\n",
        "            if word in document:\n",
        "                num_documents_containing_word += 1\n",
        "        \n",
        "        idf = math.log(total_num_documents / num_documents_containing_word)\n",
        "        idfs[word] = idf\n",
        "\n",
        "    return idfs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw_BBUv5PYuA"
      },
      "source": [
        "def top_files(query, files, idfs, n):\n",
        "    file_scores = dict()\n",
        "\n",
        "    for file, words in files.items():\n",
        "        total_tf_idf = 0\n",
        "        for word in query:\n",
        "            total_tf_idf += words.count(word) * idfs[word]\n",
        "        file_scores[file] = total_tf_idf\n",
        "\n",
        "    ranked_files = sorted(file_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ranked_files = [x[0] for x in ranked_files]\n",
        "\n",
        "    return ranked_files[:n]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XucuXiJ4PY3L"
      },
      "source": [
        "def top_sentences(query, sentences, idfs, n):\n",
        "    sentence_scores = dict()\n",
        "\n",
        "    for sentence, words in sentences.items():\n",
        "        words_in_query = query.intersection(words)\n",
        "        \n",
        "        # idf value of sentence\n",
        "        idf = 0\n",
        "        for word in words_in_query:\n",
        "            idf += idfs[word]\n",
        "        \n",
        "        # query term density of sentence\n",
        "        num_words_in_query = sum(map(lambda x: x in words_in_query, words))\n",
        "        query_term_density = num_words_in_query / len(words)\n",
        "\n",
        "        # update sentence scores with idf and query term density values\n",
        "        sentence_scores[sentence] = {'idf': idf, 'qtd': query_term_density}\n",
        "    \n",
        "    # rank sentences by idf then query term density\n",
        "    ranked_sentences = sorted(sentence_scores.items(), key=lambda x: (x[1]['idf'], x[1]['qtd']), reverse=True)\n",
        "    ranked_sentences = [x[0] for x in ranked_sentences]\n",
        "\n",
        "    return ranked_sentences[:n]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buzBnLZ2PY6U",
        "outputId": "efe25dc4-9a2e-47df-cbf9-32871575535b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: How do neurons connect in a neural network?\n",
            "Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}