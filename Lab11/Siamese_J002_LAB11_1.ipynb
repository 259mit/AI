{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese J002_LAB11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozuUcEuy66uO"
      },
      "source": [
        "#Building image pairs for siamese networks with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaxkHCow7A0D"
      },
      "source": [
        "Siamese networks are incredibly powerful networks, responsible for significant increases in face recognition, signature verification, and prescription pill identification applications (just to name a few)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQQb8CUO7UED"
      },
      "source": [
        "We use siamese networks when performing verification, identification, or recognition tasks, the most popular examples being face recognition and signature verification.\n",
        "\n",
        "For example, let’s suppose we are tasked with detecting signature forgeries. Instead of training a classification model to correctly classify signatures for each unique individual in our dataset (which would require significant training data), what if we instead took two images from our training set and asked the neural network if the signatures were from the same person or not?\n",
        "\n",
        "If the two signatures are the same, then siamese network reports “Yes”.\n",
        "Otherwise, if the two signatures are not the same, thereby implying a potential forgery, the siamese network reports “No”.\n",
        "\n",
        "This is an example of a verification task (versus classification, regression, etc.), and while it may sound like a harder problem, it actually becomes far easier in practice — we need significantly less training data, and our accuracy actually improves by using siamese networks rather than classification networks.\n",
        "\n",
        "Another added benefit is that we no longer need a “catch-all” class for when our classification model needs to select “none of the above” when making a classification (which in practice is quite error prone). Instead, our siamese network handles this problem gracefully by reporting that the two signatures are not the same.\n",
        "\n",
        "Keep in mind that the siamese network architecture doesn’t have to concern itself with classification in the traditional sense of having to select 1 of N possible classes. Rather, the siamese network just needs to be able to report “same” (belongs to the same class) or “different” (belongs to different classes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xz7MRdB7c96"
      },
      "source": [
        "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2020/11/siamese_image_pairs_signet.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRpktS9v7w1G"
      },
      "source": [
        "#The concept of “image pairs” in siamese networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hHpGdda7zDB"
      },
      "source": [
        "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2020/11/siamese_image_pairs_example.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLgtSPMY784G"
      },
      "source": [
        "After reviewing the previous section, you should understand that a siamese network consists of two subnetworks that mirror each other (i.e., when the weights update in one network, the same weights are updated in the other network).\n",
        "\n",
        "Since there are two subnetworks, we must have two inputs to the siamese model (as you saw in Figure 2 at the top of the previous section).\n",
        "\n",
        "When training siamese networks we need to have positive pairs and negative pairs:\n",
        "\n",
        "    1. Positive pairs: Two images that belong to the same class (ex., two images of the same person, two examples of the same signature, etc.)\n",
        "    2. Negative pairs: Two images that belong to different classes (ex., two images of different people, two examples of different signatures, etc.)\n",
        "\n",
        "When training our siamese network, we randomly sample examples of positive and negative pairs. These pairs serve as our training data such that the siamese network can learn similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrPrYyx560Vq"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from imutils import build_montages\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Lambda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAoTAiz29Hx0"
      },
      "source": [
        "def make_pairs(images, labels):\n",
        "\t# initialize two empty lists to hold the (image, image) pairs and\n",
        "\t# labels to indicate if a pair is positive or negative\n",
        "\tpairImages = []\n",
        "\tpairLabels = []\n",
        "    # calculate the total number of classes present in the dataset\n",
        "\t# and then build a list of indexes for each class label that\n",
        "\t# provides the indexes for all examples with a given label\n",
        "\tnumClasses = len(np.unique(labels))\n",
        "\tidx = [np.where(labels == i)[0] for i in range(0, numClasses)]\n",
        "    # loop over all images\n",
        "\tfor idxA in range(len(images)):\n",
        "\t\t# grab the current image and label belonging to the current\n",
        "\t\t# iteration\n",
        "\t\tcurrentImage = images[idxA]\n",
        "\t\tlabel = labels[idxA]\n",
        "\t\t# randomly pick an image that belongs to the *same* class\n",
        "\t\t# label\n",
        "\t\tidxB = np.random.choice(idx[label])\n",
        "\t\tposImage = images[idxB]\n",
        "\t\t# prepare a positive pair and update the images and labels\n",
        "\t\t# lists, respectively\n",
        "\t\tpairImages.append([currentImage, posImage])\n",
        "\t\tpairLabels.append([1])\n",
        "  \t\t# grab the indices for each of the class labels *not* equal to\n",
        "\t\t# the current label and randomly pick an image corresponding\n",
        "\t\t# to a label *not* equal to the current label\n",
        "\t\tnegIdx = np.where(labels != label)[0]\n",
        "\t\tnegImage = images[np.random.choice(negIdx)]\n",
        "\t\t# prepare a negative pair of images and update our lists\n",
        "\t\tpairImages.append([currentImage, negImage])\n",
        "\t\tpairLabels.append([0])\n",
        "\t# return a 2-tuple of our image pairs and labels\n",
        "\treturn (np.array(pairImages), np.array(pairLabels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXUUPAYg96iD"
      },
      "source": [
        "#Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXO6eTw098VS"
      },
      "source": [
        "# load MNIST dataset and scale the pixel values to the range of [0, 1]\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "trainX = trainX / 255.0\n",
        "testX = testX / 255.0\n",
        "# add a channel dimension to the images\n",
        "\n",
        "\n",
        "# build the positive and negative image pairs\n",
        "print(\"[INFO] preparing positive and negative pairs...\")\n",
        "(pairTrain, labelTrain) = make_pairs(trainX, trainY)\n",
        "(pairTest, labelTest) = make_pairs(testX, testY)\n",
        "# initialize the list of images that will be used when building our\n",
        "# montage\n",
        "images = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJUYr63Z_lBH"
      },
      "source": [
        "pairTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p6rwv-5HpTF"
      },
      "source": [
        "pairTrain[0,0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJswrbOJSOG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(pairTrain[0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR5WZUEHJfah"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(pairTrain[0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NH8cuYXJjOd"
      },
      "source": [
        "labelTrain[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EepJsogP_ML1"
      },
      "source": [
        "# Loop over a sample of our training pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLqm2Yad-Kp3"
      },
      "source": [
        "\n",
        "for i in np.random.choice(np.arange(0, len(pairTrain)), size=(49,)):\n",
        "\t# grab the current image pair and label\n",
        "\timageA = pairTrain[i][0]\n",
        "\timageB = pairTrain[i][1]\n",
        "\tlabel = labelTrain[i]\n",
        "\t# to make it easier to visualize the pairs and their positive or\n",
        "\t# negative annotations, we're going to \"pad\" the pair with four\n",
        "\t# pixels along the top, bottom, and right borders, respectively\n",
        "\toutput = np.zeros((36, 60), dtype=\"uint8\")\n",
        "\tpair = np.hstack([imageA, imageB])\n",
        "\toutput[4:32, 0:56] = pair\n",
        "\t# set the text label for the pair along with what color we are\n",
        "\t# going to draw the pair in (green for a \"positive\" pair and\n",
        "\t# red for a \"negative\" pair)\n",
        "\ttext = \"neg\" if label[0] == 0 else \"pos\"\n",
        "\tcolor = (0, 0, 255) if label[0] == 0 else (0, 255, 0)\n",
        "\t# create a 3-channel RGB image from the grayscale pair, resize\n",
        "\t# it from 60x36 to 96x51 (so we can better see it), and then\n",
        "\t# draw what type of pair it is on the image\n",
        "\tvis = cv2.merge([output] * 3)\n",
        "\tvis = cv2.resize(vis, (96, 51), interpolation=cv2.INTER_LINEAR)\n",
        "\tcv2.putText(vis, text, (2, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
        "\t\tcolor, 2)\n",
        "\t# add the pair visualization to our list of output images\n",
        "\timages.append(vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmg22ehE_GAe"
      },
      "source": [
        "#Display positive and negative pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl-sEGKl-32J"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# construct the montage for the images\n",
        "montage = build_montages(images, (96, 51), (7, 7))[0]\n",
        "# show the output montage\n",
        "cv2_imshow(montage)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCCIUIssAY_8"
      },
      "source": [
        "#Siamese network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbYc2ufh_cud"
      },
      "source": [
        "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2020/11/keras_siamese_networks_header.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yekR16K_AVvh"
      },
      "source": [
        "    1. A method used to generate image pairs such that we can train our siamese network\n",
        "    2. A custom CNN layer to compute Euclidean distances between vectors inside of the network\n",
        "    3. A utility used to plot the siamese network training history to disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_UvQ1KBAiCZ"
      },
      "source": [
        "#What are siamese networks and how do they work?\n",
        "\n",
        "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2020/11/keras_siamese_networks_process.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-94Mn0fAfpo"
      },
      "source": [
        "A basic siamese network architecture implementation accepts two input images (left), has identical CNN subnetworks for each input with each subnetwork ending in a fully-connected layer (middle), computes the Euclidean distance between the fully-connected layer outputs, and then passes the distance through a sigmoid activation function to determine similarity (right)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNjRXphP_fnN"
      },
      "source": [
        "# import the necessary packages\n",
        "import os\n",
        "# specify the shape of the inputs for our network\n",
        "IMG_SHAPE = (28, 28, 1)\n",
        "# specify the batch size and number of epochs\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "# define the path to the base output directory\n",
        "BASE_OUTPUT = \"output\"\n",
        "# use the base output path to derive the path to the serialized\n",
        "# model along with training history plot\n",
        "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"siamese_model\"])\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvrj-X2DBBVH"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQg622EBbGa"
      },
      "source": [
        "#Build siamese model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9GTyrYBBGg8"
      },
      "source": [
        "def build_siamese_model(inputShape, embeddingDim=48):\n",
        "\t# specify the inputs for the feature extractor network\n",
        "\tinputs = Input(inputShape)\n",
        "\t# define the first set of CONV => RELU => POOL => DROPOUT layers\n",
        "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(inputs)\n",
        "\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\tx = Dropout(0.3)(x)\n",
        "\t# second set of CONV => RELU => POOL => DROPOUT layers\n",
        "\tx = Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "\tx = MaxPooling2D(pool_size=2)(x)\n",
        "\tx = Dropout(0.3)(x)\n",
        " \t# prepare the final outputs\n",
        "\tpooledOutput = GlobalAveragePooling2D()(x)\n",
        "\toutputs = Dense(embeddingDim)(pooledOutput)\n",
        "\t# build the model\n",
        "\tmodel = Model(inputs, outputs)\n",
        "\t# return the model to the calling function\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfHra3IFDJ5Z"
      },
      "source": [
        "Our next function, euclidean_distance\n",
        ", accepts a 2-tuple of vectors\n",
        "and then computes the Euclidean distance between them, utilizing Keras/TensorFlow functions to do so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03TmWYUqDLrb"
      },
      "source": [
        "def euclidean_distance(vectors):\n",
        "\t# unpack the vectors into separate lists\n",
        "\t(featsA, featsB) = vectors\n",
        "\t# compute the sum of squared distances between the vectors\n",
        "\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
        "\t\tkeepdims=True)\n",
        "\t# return the euclidean distance between the vectors\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlaO6QcADT3Z"
      },
      "source": [
        "Our final function, plot_training\n",
        ", accepts (1) the training history from calling model.fit\n",
        "and (2) an output plotPath\n",
        ":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf0nP1_TDUwt"
      },
      "source": [
        "def plot_training(H, plotPath):\n",
        "\t# construct a plot that plots and saves the training history\n",
        "\tplt.style.use(\"ggplot\")\n",
        "\tplt.figure()\n",
        "\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
        "\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
        "\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
        "\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "\tplt.title(\"Training Loss and Accuracy\")\n",
        "\tplt.xlabel(\"Epoch #\")\n",
        "\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\tplt.legend(loc=\"lower left\")\n",
        "\tplt.savefig(plotPath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5yL8oVbGPjJ"
      },
      "source": [
        "#Building siamese network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQpxgDtlE9bO"
      },
      "source": [
        "print(\"[INFO] building siamese network...\")\n",
        "imgA = Input(shape=IMG_SHAPE)\n",
        "imgB = Input(shape=IMG_SHAPE)\n",
        "featureExtractor = build_siamese_model(IMG_SHAPE)\n",
        "featsA = featureExtractor(imgA)\n",
        "featsB = featureExtractor(imgB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU3pkpcrFSsg"
      },
      "source": [
        "# finally, construct the siamese network\n",
        "distance = Lambda(euclidean_distance)([featsA, featsB])\n",
        "outputs = Dense(1, activation=\"sigmoid\")(distance)\n",
        "model = Model(inputs=[imgA, imgB], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbRJOYFcGyvb"
      },
      "source": [
        "#Now that our siamese network architecture is constructed, we can move on to training it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQiQ8jEiGzcg"
      },
      "source": [
        "# compile the model\n",
        "print(\"[INFO] compiling model...\")\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "history = model.fit(\n",
        "\t[pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:],\n",
        "\tvalidation_data=([pairTest[:, 0], pairTest[:, 1]], labelTest[:]),\n",
        "\tbatch_size=BATCH_SIZE, \n",
        "\tepochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baw7P60rHJEX"
      },
      "source": [
        "#Once the model is trained, we can serialize it to disk and plot the training history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdIxwcjMHJ51"
      },
      "source": [
        "# serialize the model to disk\n",
        "print(\"[INFO] saving siamese model...\")\n",
        "model.save(MODEL_PATH)\n",
        "# plot the training history\n",
        "print(\"[INFO] plotting training history...\")\n",
        "plot_training(history, PLOT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqikEtqLzMM"
      },
      "source": [
        "#    “How can we use our trained siamese network to predict the similarity between two images?”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akX6nShiLxvN"
      },
      "source": [
        "\n",
        "\n",
        "The answer is that we utilize the final layer in our siamese network implementation, which is sigmoid activation function.\n",
        "\n",
        "The sigmoid activation function has an output in the range [0, 1], meaning that when we present an image pair to our siamese network, the model will output a value >= 0 and <= 1.\n",
        "\n",
        "A value of 0\n",
        "means that the two images are completely and totally dissimilar, while a value of 1\n",
        "implies that the images are very similar.\n",
        "\n",
        "An example of such a similarity can be seen in Figure 1 at the top of this section:\n",
        "\n",
        "    Comparing a “7” to a “0” has a low similarity score of only 0.02.\n",
        "    However, comparing a “0” to another “0” has a very high similarity score of 0.93.\n",
        "\n",
        "A good rule of thumb is to use a similarity cutoff value of 0.5\n",
        "(50%) as your threshold:\n",
        "\n",
        "    If two image pairs have an image similarity of <= 0.5, then they belong to a different class.\n",
        "    Conversely, if pairs have a predicted similarity of > 0.5, then they belong to the same class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iW9YcyiNXUW"
      },
      "source": [
        "type(pairTrain[0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hQowd2sNE3d"
      },
      "source": [
        "# add channel a dimension to both the images\n",
        "imageA = np.expand_dims(pairTrain[0, 0], axis=-1)\n",
        "imageB = np.expand_dims(pairTrain[0, 0], axis=-1)\n",
        "# add a batch dimension to both images\n",
        "imageA = np.expand_dims(imageA, axis=0)\n",
        "imageB = np.expand_dims(imageB, axis=0)\n",
        "preds = model.predict([imageA, imageB])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_nl58tIN1Jh"
      },
      "source": [
        "preds[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaFyeS6iOsJY"
      },
      "source": [
        "imageA = np.resize(imageA,(28,28))\n",
        "imageB = np.resize(imageB,(28,28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR6Fpv8cOAHo"
      },
      "source": [
        "\t# initialize the figure\n",
        "\tfig = plt.figure(\"Pair #{}\".format(i + 1), figsize=(4, 2))\n",
        "\tplt.suptitle(\"Similarity: {:.2f}\".format(preds[0][0]))\n",
        "\t# show first image\n",
        "\tax = fig.add_subplot(1, 2, 1)\n",
        "\tplt.imshow(imageA, cmap=plt.cm.gray)\n",
        "\tplt.axis(\"off\")\n",
        "\t# show the second image\n",
        "\tax = fig.add_subplot(1, 2, 2)\n",
        "\tplt.imshow(imageB, cmap=plt.cm.gray)\n",
        "\tplt.axis(\"off\")\n",
        "\t# show the plot\n",
        "\tplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}